{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installation of Pre-Requisite Libraries**"
      ],
      "metadata": {
        "id": "-38o5Wu4nAgQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vn3AA9kfGWs"
      },
      "outputs": [],
      "source": [
        "!pip install pandas requests beautifulsoup4 scikit-learn numpy lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scraping Using BeautifulSoup**"
      ],
      "metadata": {
        "id": "uZWNRiNOnG1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we scrapped Mayo Clinic Diseases by Alphabet Letter and Save to a CSV file\n",
        "\n",
        "This script iterates through all letters A-Z and scrapes disease names and their URLs from the Mayo Clinic website. For each letter, it fetches the corresponding diseases page, extracts the disease links, and writes the results (a letter, disease name, and URL) to a CSV file."
      ],
      "metadata": {
        "id": "1Vei5I51nLMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRv8pxAUfVGE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from csv import writer\n",
        "import string\n",
        "\n",
        "\n",
        "with open('new_mayo_clinic_diseases.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    csv_writer = writer(f)\n",
        "    csv_writer.writerow(['Letter', 'Disease Name','URL'])\n",
        "\n",
        "\n",
        "    for letter in string.ascii_uppercase:\n",
        "        while True:\n",
        "            url = f\"https://www.mayoclinic.org/diseases-conditions/index?letter={letter}\"\n",
        "            page = requests.get(url)\n",
        "            soup = BeautifulSoup(page.content, 'html.parser')\n",
        "            disease_links = soup.find_all('a', class_='cmp-anchor--plain cmp-button cmp-button__link cmp-result-name__link')\n",
        "\n",
        "            if not disease_links:\n",
        "                break\n",
        "\n",
        "            for link in disease_links:\n",
        "                url1 = link.get('href')\n",
        "                disease_name = link.text.strip()\n",
        "                csv_writer.writerow([letter, disease_name,url1])\n",
        "                print(f\"Letter {letter} - Added: {disease_name} and URL\")\n",
        "\n",
        "            next_page = soup.find('a', {'aria-label': 'Next page'})\n",
        "            if not next_page:\n",
        "                break\n",
        "\n",
        "print(\"Scraping completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the Symptoms**"
      ],
      "metadata": {
        "id": "OsUg4hGarK6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapping the associated symptom of each disease from the csv file and using the associated URL to create an new CSV."
      ],
      "metadata": {
        "id": "oycnu-fyrMtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TyMAgPNZfeqW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "df = pd.read_csv('new_mayo_clinic_diseases.csv')\n",
        "\n",
        "def scrape_symptoms(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        symptoms_header = soup.find('h2', string='Symptoms')\n",
        "        if symptoms_header:\n",
        "            symptoms_text = symptoms_header.find_next('ul').get_text(separator=' ', strip=True)\n",
        "            return symptoms_text\n",
        "        return \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "df['Symptoms'] = df['URL'].apply(scrape_symptoms)\n",
        "df.to_csv('diseases_with_symptoms.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Rarity**"
      ],
      "metadata": {
        "id": "5nWqKu8Ssc80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding additional extra rare criteria which will give the disease lesser probability to occur."
      ],
      "metadata": {
        "id": "BIPbr1P4sjAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('new_mayo_clinic_diseases.csv')\n",
        "\n",
        "def scrape_symptoms_and_rarity(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        symptoms_header = soup.find('h2', string='Symptoms')\n",
        "        overview_header = soup.find('h2', string='Overview')\n",
        "\n",
        "        symptoms = []\n",
        "        if symptoms_header:\n",
        "            for li in symptoms_header.find_next('ul').find_all('li'):\n",
        "                symptoms.append(li.get_text(strip=True))\n",
        "\n",
        "        if overview_header and symptoms_header:\n",
        "            content = ''\n",
        "            current = overview_header.find_next()\n",
        "            while current and current != symptoms_header:\n",
        "                if current.name == 'p':\n",
        "                    content += current.get_text() + ' '\n",
        "                current = current.find_next()\n",
        "            if re.search(r'\\b(rare|uncommon|rarely)\\b', content.lower()):\n",
        "                is_rare = 1\n",
        "\n",
        "        return symptoms, is_rare\n",
        "    except:\n",
        "        return [], 0\n",
        "\n",
        "df[['Symptoms', 'IsRare']] = df['URL'].apply(\n",
        "    lambda x: pd.Series(scrape_symptoms_and_rarity(x))\n",
        ")\n",
        "df.to_csv('prefinal_diseases_with_symptoms_enhanced.csv', index=False)\n",
        ""
      ],
      "metadata": {
        "id": "d0hxviWkjrcm"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}